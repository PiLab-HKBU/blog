---
title: "CIKM 2020 | 神经模板——给推荐的产品自动生成模板状解释及其评估"
index: true
icon: edit
author: ["Lei Li"]
sidebar: false
editLink: false
category:
- Recommender Systems
- Natural Language Processing
---

*Author: [Lei LI](https://lileipisces.github.io/)*

*Department of Computer Science, Hong Kong Baptist University*

- [Lei Li](https://lileipisces.github.io/), [Yongfeng Zhang](http://yongfeng.me/), [Li Chen](https://www.comp.hkbu.edu.hk/~lichen/). Generate Neural Template Explanations for Recommendation. CIKM'20. \[[Paper](https://doi.org/10.1145/3340531.3411992)\]\[[Code](https://github.com/lileipisces/NETE)\]\[[Toolkit](https://github.com/lileipisces/Sentires-Guide)\]\[[Datasets](https://lifehkbueduhk-my.sharepoint.com/:f:/g/personal/16484134_life_hkbu_edu_hk/Eln600lqZdVBslRwNcAJL5cBarq6Mt8WzDKpkq1YCqQjfQ?e=cISb1C)\]
- [Lei Li](https://lileipisces.github.io/), [Li Chen](https://www.comp.hkbu.edu.hk/~lichen/), [Yongfeng Zhang](http://yongfeng.me/). Towards Controllable Explanation Generation for Recommender Systems via Neural Template. WWW'20 Demo. \[[Paper](https://doi.org/10.1145/3366424.3383540)\]

## 摘要

个性化的推荐系统在这个信息过载的时代能够有效的协助用户做决策。同时，对推荐进行解释可以进一步帮助用户更好地理解推荐的产品，以便做出明智的选择，这就让可解释性推荐这个课题变得尤其重要。由于文本在向用户传达丰富信息方面的优势，基于文本的解释成为推荐系统的重要解释形式。但是，当前用于产生文本解释的方法要么局限于预先定义的模板，这会限制句子的表达能力；要么采用文本生成技术产生丰富多样的句子，这使得文本质量变得难以控制。为了兼顾文本的表达能力和质量，我们提出一种神经模板（Neural Template，NETE）解释生成框架，该框架通过从数据中自动学出模板，并针对产品特定属性来生成解释，达到两全其美的目的。真实数据集上的实验结果表明，NETE在文本表达能力和质量方面都优于最新的解释生成方法。进一步的案例分析还显示了NETE在生成多样且可控的解释方面的优势。

## 问答

### 能否用几个简单的例子概括一下你们的论文跟相关工作的不同?

如图1所示，预先定义的模板（如，CF和EFM）对于不同的推荐千篇一律地采用同一种解释，但这没法有效地反映出推荐产品的特色。比如，EFM说某产品的某个属性好（performs well），但又没法说出个所以然来。文本生成技术所产生的解释丰富多样，固然可以提高文本的表达能力。但是，生成的文本常常会出现“文不对题”的情况。比如，在图2中Att2Seq产生的两个句子都跟参照文本毫不相干。文本生成还有一个问题是经常产生近似或完全一样的句子，如NRT在两个不同案例上产生的相同句子。而我们的方法NETE有效的解决了上述问题，产生的解释就如同将某个产品属性填入模板一样，并且这些模板是从数据里学出来的，省去了人工，而且形式不固定，多种多样，形神兼备。这也正是我们把它命名为神经模板（Neural Template）的原因。

![img](https://picx.zhimg.com/80/v2-20eba05dcdbf5d20e23477b8e18a3917_1440w.png?source=d16d100b)
Figure 1: 预先定义的模板.

![img](https://pic1.zhimg.com/80/v2-2c1bf498112cedd8af76e2fad48f5f76_1440w.png?source=d16d100b)
Figure 2: 文本生成技术产生的解释.

### 是否可以用几句话简要概括一下你们的模型？

如图3所示，我们有推荐和解释两个任务。推荐任务通过MLP实现预测评分的功能。解释任务基于encoder-decoder，其中encoder是一个MLP，而decoder是我们改进的一个RNN，叫做GFRU（图4）。GFRU包括两个GRU和一个GFU，其中一个GRU负责处理产品属性（如ramen），而另一个GRU负责其他所有词（我们称之为模板），最后GFU则将两种信息进行融合。生成解释的过程跟普通RNN无异。

![img](https://pic1.zhimg.com/80/v2-0635a825f3136364766f69348b339507_1440w.png?source=d16d100b)
Figure 3: NETE框架图.

![img](https://pic1.zhimg.com/80/v2-6149996d0cfef466a6a6cbc671770235_1440w.png?source=d16d100b)
Figure 4: GFRU结构图.

### 你们的训练数据需要进行特殊处理吗？

为了获得高质量的解释性文本数据，我们通过一个情感分析工具[Sentires](https://github.com/evison/Sentires)从用户评论中抽取了所有包含产品属性的句子。我们认为这种包含具体产品属性的句子信息量更大，跟产品也更相关。相关工作中有采用整个用户评论、评论的第一句话或评论的标题作为训练数据的，但是这样会使数据包含噪音，产生跟推荐完全无关的解释，就像上述例子中展示的一样。为了推动可解释性推荐的发展，我们已经公开了我们的源代码和数据集。

### 你们是如何评估解释性的？

我们既采用了自然语言处理领域中常用的ROUGE和BLEU，还提出了4种新的评价标准，因为我们注意到ROUGE和BLEU这种评估文本相似度的评价方式不能涵盖“解释性”的方方面面。比如，我们发现NRT在整个数据集上产生的99%的句子一模一样，而Att2Seq的结果比较多样，但是它们在ROUGE和BLEU上的分数却几乎一样。所以我们设计了一个评估句子重复性的标准。其他标准还包括产品属性的匹配率，属性的覆盖率和属性的多样性。提出这几种评估指标的出发点是希望模型能够生成具体到产品属性的句子，并且句子及其中的属性应尽可能地多样化。

除了自动评估外，我们还请了10个人做了一个简单的用户调查。结果（图5）显示我们的方法NETE产生的解释比Att2Seq更接近参照文本，并且对属性的解释很到位。这部分的结果放在我们的Demo论文上。

![img](https://picx.zhimg.com/80/v2-3a82912885db34d91b02979fc86b7b43_1440w.png?source=d16d100b)
Figure 5: 用户调查结果.

### 为什么一个工作发在两个地方？

说来实在是惭愧，因为写作功力欠佳，这个工作跟审稿人来了几个回合，前后耗时一年。结果篇幅越改越长，不得不进行拆分。不过两篇论文的内容并不重合，WWW'20这篇只有用户调查的结果和Demo的展示。关于Demo的具体功能和搭建过程可以访问下面这个链接。

[Tensorflow训练好的模型如何用web部署?](https://www.zhihu.com/question/385091513/answer/1130184438)